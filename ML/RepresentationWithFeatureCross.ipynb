{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6143a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# The following lines adjust the granularity of reporting.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "print(\"Imported the modules.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0731afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
    "test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")\n",
    "\n",
    "# Scale the labels\n",
    "scale_factor = 1000.0\n",
    "# Scale the training set's label.\n",
    "train_df[\"median_house_value\"] /= scale_factor\n",
    "\n",
    "# Scale the test set's label\n",
    "test_df[\"median_house_value\"] /= scale_factor\n",
    "\n",
    "# Shuffle the examples\n",
    "train_df = train_df.reindex(np.random.permutation(train_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44a5de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    'latitude':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='latitude'),\n",
    "    'longitude':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='longitude')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fbb42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(my_inputs, my_outputs, my_learning_rate):\n",
    "\n",
    "  model = tf.keras.Model(inputs=my_inputs, outputs=my_outputs)\n",
    "\n",
    "  # Construct the layers into a model that TensorFlow can execute.\n",
    "  model.compile(optimizer=tf.keras.optimizers.experimental.RMSprop(\n",
    "      learning_rate=my_learning_rate),\n",
    "      loss=\"mean_squared_error\",\n",
    "      metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def train_model(model, dataset, epochs, batch_size, label_name):\n",
    "  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "\n",
    "  features = {name:np.array(value) for name, value in dataset.items()}\n",
    "  label = np.array(features.pop(label_name))\n",
    "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True)\n",
    "\n",
    "  # The list of epochs is stored separately from the rest of history.\n",
    "  epochs = history.epoch\n",
    "\n",
    "  # Isolate the mean absolute error for each epoch.\n",
    "  hist = pd.DataFrame(history.history)\n",
    "  rmse = hist[\"root_mean_squared_error\"]\n",
    "\n",
    "  return epochs, rmse\n",
    "\n",
    "\n",
    "def plot_the_loss_curve(epochs, rmse):\n",
    "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Root Mean Squared Error\")\n",
    "\n",
    "  plt.plot(epochs, rmse, label=\"Loss\")\n",
    "  plt.legend()\n",
    "  plt.ylim([rmse.min()*0.94, rmse.max()* 1.05])\n",
    "  plt.show()\n",
    "\n",
    "print(\"Defined the create_model, train_model, and plot_the_loss_curve functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "label_name = 'median_house_value'\n",
    "\n",
    "# The two Input layers are concatenated so they can be passed as a single\n",
    "# tensor to a Dense layer.\n",
    "preprocessing_layer = tf.keras.layers.Concatenate()(inputs.values())\n",
    "\n",
    "dense_output = layers.Dense(units=1, name='dense_layer')(preprocessing_layer)\n",
    "\n",
    "outputs = {\n",
    "  'dense_output': dense_output\n",
    "}\n",
    "\n",
    "# Create and compile the model's topography.\n",
    "my_model = create_model(inputs, outputs, learning_rate)\n",
    "\n",
    "# To view a PNG of this model's layers, uncomment the call to\n",
    "# `tf.keras.utils.plot_model` below. After running this code cell, click\n",
    "# the file folder on the left, then the `my_model.png` file.\n",
    "# tf.keras.utils.plot_model(my_model, \"my_model.png\", show_shapes=True)\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, rmse = train_model(my_model, train_df, epochs, batch_size, label_name)\n",
    "\n",
    "# Print out the model summary.\n",
    "my_model.summary(expand_nested=True)\n",
    "\n",
    "plot_the_loss_curve(epochs, rmse)\n",
    "\n",
    "print(\"\\n: Evaluate the new model against the test set:\")\n",
    "test_features = {name:np.array(value) for name, value in test_df.items()}\n",
    "test_label = np.array(test_features.pop(label_name))\n",
    "my_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe8c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution_in_degrees = 1.0\n",
    "\n",
    "# Create a list of numbers representing the bucket boundaries for latitude.\n",
    "latitude_boundaries = list(np.arange(int(min(train_df['latitude'])),\n",
    "                                     int(max(train_df['latitude'])),\n",
    "                                     resolution_in_degrees))\n",
    "print(\"latitude boundaries: \" + str(latitude_boundaries))\n",
    "\n",
    "# Create a Discretization layer to separate the latitude data into buckets.\n",
    "latitude = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=latitude_boundaries,\n",
    "    name='discretization_latitude')(inputs.get('latitude'))\n",
    "\n",
    "# Number of categories is the length of latitude_boundaries plus one.\n",
    "latitude = tf.keras.layers.CategoryEncoding(\n",
    "    num_tokens=len(latitude_boundaries) + 1,\n",
    "    output_mode='one_hot',\n",
    "    name='category_encoding_latitude')(latitude)\n",
    "\n",
    "# Create a list of numbers representing the bucket boundaries for longitude.\n",
    "longitude_boundaries = list(np.arange(int(min(train_df['longitude'])),\n",
    "                                      int(max(train_df['longitude'])),\n",
    "                                      resolution_in_degrees))\n",
    "\n",
    "print(\"longitude boundaries: \" + str(longitude_boundaries))\n",
    "\n",
    "# Create a Discretization layer to separate the longitude data into buckets.\n",
    "longitude = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=longitude_boundaries,\n",
    "    name='discretization_longitude')(inputs.get('longitude'))\n",
    "\n",
    "# Number of categories is the length of longitude_boundaries plus one.\n",
    "longitude = tf.keras.layers.CategoryEncoding(\n",
    "    num_tokens=len(longitude_boundaries) + 1,\n",
    "    output_mode='one_hot',\n",
    "    name='category_encoding_longitude')(longitude)\n",
    "\n",
    "# Concatenate latitude and longitude into a single tensor as input for the Dense layer.\n",
    "concatenate_layer = tf.keras.layers.Concatenate()([latitude, longitude])\n",
    "\n",
    "dense_output = layers.Dense(units=1, name='dense_layer')(concatenate_layer)\n",
    "\n",
    "# Define an output dictionary we'll send to the model constructor.\n",
    "outputs = {\n",
    "  'dense_output': dense_output\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e611b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.04\n",
    "epochs = 35\n",
    "\n",
    "# Build the model.\n",
    "my_model = create_model(inputs, outputs, learning_rate)\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, rmse = train_model(my_model, train_df, epochs, batch_size, label_name)\n",
    "\n",
    "# Print out the model summary.\n",
    "my_model.summary(expand_nested=True)\n",
    "\n",
    "plot_the_loss_curve(epochs, rmse)\n",
    "\n",
    "print(\"\\n: Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc28275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resolution_in_degrees = 1.0\n",
    "resolution_in_degrees = 0.4\n",
    "\n",
    "# Create a list of numbers representing the bucket boundaries for latitude.\n",
    "latitude_boundaries = list(np.arange(int(min(train_df['latitude'])),\n",
    "                                     int(max(train_df['latitude'])),\n",
    "                                     resolution_in_degrees))\n",
    "\n",
    "# Create a Discretization layer to separate the latitude data into buckets.\n",
    "latitude = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=latitude_boundaries,\n",
    "    name='discretization_latitude')(inputs.get('latitude'))\n",
    "\n",
    "# Create a list of numbers representing the bucket boundaries for longitude.\n",
    "longitude_boundaries = list(np.arange(int(min(train_df['longitude'])),\n",
    "                                      int(max(train_df['longitude'])),\n",
    "                                      resolution_in_degrees))\n",
    "\n",
    "# Create a Discretization layer to separate the longitude data into buckets.\n",
    "longitude = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=longitude_boundaries,\n",
    "    name='discretization_longitude')(inputs.get('longitude'))\n",
    "\n",
    "# Cross the latitude and longitude features into a single one-hot vector.\n",
    "feature_cross = tf.keras.layers.HashedCrossing(\n",
    "    num_bins=len(latitude_boundaries) * len(longitude_boundaries),\n",
    "    output_mode='one_hot',\n",
    "    name='cross_latitude_longitude')([latitude, longitude])\n",
    "\n",
    "dense_output = layers.Dense(units=1, name='dense_layer')(feature_cross)\n",
    "\n",
    "# Define an output dictionary we'll send to the model constructor.\n",
    "outputs = {\n",
    "  'dense_output': dense_output\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb63f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.04\n",
    "epochs = 35\n",
    "\n",
    "# Build the model, this time passing in the feature_cross_feature_layer:\n",
    "my_model = create_model(inputs, outputs, learning_rate)\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, rmse = train_model(my_model, train_df, epochs, batch_size, label_name)\n",
    "\n",
    "# Print out the model summary.\n",
    "my_model.summary(expand_nested=True)\n",
    "\n",
    "plot_the_loss_curve(epochs, rmse)\n",
    "\n",
    "print(\"\\n: Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daa3d07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
